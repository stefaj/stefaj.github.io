<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>My Hakyll Blog - Hidden Markov Models</title>
        <!--<link rel="stylesheet" type="text/css" href="/css/default.css" /> -->
		
		
		<link rel="stylesheet" href="../css/bootstrap.min.css">

		<!-- Optional theme -->
		<link rel="stylesheet" href="../css/bootstrap-theme.min.css">

		<!-- Latest compiled and minified JavaScript -->
		<script src="../js/jquery-1.11.3.min.js"></script>
		<script src="../js/bootstrap.min.js"></script>
		
		<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


    </head>
    <body>
		
			<nav class="navbar navbar-inverse navbar-fixed-top">
				  <div class="container">
					<div class="navbar-header" id="header">
					  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
						<span class="sr-only">Toggle navigation</span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					  </button>
					  <a class="navbar-brand" id="logo" href="#">Musings of Stefan J</a>
					</div>
					<div id="navbar" class="collapse navbar-collapse">
					  <ul class="nav navbar-nav">
						<li><a href="../">Home</a></li>
						<li><a href="../about.html">About</a></li>
						<li><a href="../contact.html">Contact</a></li>
						<li><a href="../archive.html">Archive</a></li>
					  </ul>
					</div><!--/.nav-collapse -->
				  </div>
			</nav>
			
			
		<br><br>


		<div class="container">
			<div class="rowaa">
				<div id="content">
					<h1>Hidden Markov Models</h1>

					<div class="info">
    
    
</div>

<h2 id="introduction">Introduction</h2>
<p>HMM are stochastic methods that are used to model sequential data such as speech and gesture recognition. Both the underlying process which is hidden is stochastic and the observable process. Only a first order HMM will be considered.</p>
<p>Under the Markov property, the next state is only dependent on the current state of the system. Such states may not be known and may be hidden from the observer as only the output values are observable. When an event is generated from a state, the model moves into a new state based on its transition probabilities. The term hidden is commonly used to indicate that many different state sequences can generate the same observed sequence of events.</p>
<p>The HMM is formally defined by the triple: \[\lambda = (A,B,\pi) \]</p>
<p>where A is the transition matrix, B the emission matrix and \( \pi \) the initial state vector. Let \(S\) be the set of states, \(V\) the set of observations, \(Q\) a fixed state sequence of length \(T\) and \(O\) a corresponding observation sequence. The transition probabilities are then given by: \[ A = P(q_t = s_j | q_{t-1} = s_i) \] and the emission probabilities are given by: \[ B = P(o_t = v_k | q_t = s_i) \] and the initial state probabilities by: \[ \pi = P(q_1 = s_i) \]</p>
<p>Given a sequence of observations, we want to compute the probability of an observation sequence given the model.</p>
<p>The probability of the observation sequence \( O \) for a state sequence is given by: \[ P(O|Q,\lambda) = \prod^T_{t=1} P(o_t | q_t,\lambda) \] The probability of the state sequence is simply a product of the state path: \[ P(Q|\lambda)= \pi_{q_1}a_{q_1 q_2} \cdots a_{q_{T-1} q_T} \] Thus the probability of the observations given the model can be calculated by: \[ P(O|\lambda) = \sum_Q P(O|Q,\lambda) P(Q|\lambda) \]</p>
<p>The forward algorithm calculates this efficiently by caching previous calculations.</p>
<h2 id="training">Training</h2>
<p>A variety of learning algorithms exist which compute the structure of the model and also calculate the emission and transmission matrices. The Baum-Welch is an unsupervised learning algorithm which re-estimates the parameters \( (A,B,\pi) \). The training problem can be solved in terms of joint events and state variables.</p>
<p>The joint variable \( \xi_t(i,j) = P(q_t = S_i, q_{t+1} = S_j|S, \lambda) \) is given by: \[ \frac{\alpha_t (j) a_{ij} b_j (O_{t+1}) \beta_{t+1}(j) }{\sum_{k=1}^N \sum_{l=1}^N \alpha_t (k) a_{kl} b_l (O_{t+1}) \beta_{t+1}(l) } \]</p>
<p>The state variable \( \gamma_t(i) = P(q_t = S_i | O, \lambda) \) is given by: \[ \gamma_t(i) = \frac{\alpha_t (i) \beta_t(i)}{\sum_{j=1}^N \alpha_t (j) \beta_t (j)} \]</p>
<p>Upon which the parameters can be updated. The updated state transition probability is then given by: \[ a^u_{ij} = \frac{\sum_{t=1}^{T-1} \xi_t (i,j)}{\sum_{t=1}^{T-1} \gamma_t (i)} \]</p>
<p>The updated emission probability is given by: \[ b^u_j (k) = \frac{\sum^T_{t=1,o_t = v_k} \gamma_t (j)}{\sum^T_{t=1} \gamma_t (j)} \] The updated initial probability by: \[ \pi^u_i = \gamma_1 (i) \]</p>
<h2 id="composition">Composition</h2>
<p>HMMs have been applied with some degree of success to music composition. A system has been developed for producing a counterpoint line to a cantus firmis in the style of Palestrina Farbood2001. Another approach utilizes a HMM for chorale melody harmonization Allan2004.</p>

				</div>
			</div>
		</div>
		
		   <footer class="footer">
		  <div class="container">
		   <p class="text-muted">
			Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
			</p>
		  </div>
		</footer>
    </body>
</html>
